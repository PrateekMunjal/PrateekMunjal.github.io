---
layout: post
title: Generative Adversarial Networks
---

In this post, we will be discussing the recent (precisely year 2014) approach, Generative Adversarial Networks (aka GAN), in generative modelling world. GANs are highly appreciated in machine learning community as it achieves the state of the art result.

To visualize the power of GANs, in practice we use the image datasets to train the model and then expect the model to output the images which _looks real_. It is very evident that the property of an image, i.e _looks real?_ depends from person to person. Thus, till date there is no unbiased and standard measure quntifying this _<u>looks real</u>_ property. 

From here we will call this _looks real_ property, as visual perception. We will discuss the issues and attempts made to solve the problem of **quantifying the visual perception** in a separate post. 

# Problem Statement
We would like to learn a function _f_, which can produce *some* output which looks real to humans. For example: generating the face of a celebrity which may or may not exist.

# GAN Model
GANs model comprises of two networks, namely Generator and Discriminator networks. We will use shorthand notations for Generator as **Gen** and for Discriminator as **Disc**. 

## Generator Network

As the name suggests this network helps us in achieving the generations.

Bookmark this! -- I will be writing iteratively :)